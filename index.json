[{"authors":["admin"],"categories":null,"content":"I am a Master\u0026rsquo;s student at the University of Chile, interested in Software Engineering, Machine Learning and Data Science. My Master\u0026rsquo;s thesis is about the Logical Interpretability of Graph Neural Networks, figuring out what is happening inside the network in the domain of formal logic.\nMy experience consists mostly of research on uses of Machine Learning on source code and research of Graph Neural Networks, and implementing various prototypes for freelance and research projects. This ranges from EDA and feature and data engineering, to software development and data visualization. I also have experience in teaching as a TA for several courses in University, ranging from advanced programming methodology classes to data mining and deep learning. Currently I am working as a TA for the following courses: Deep Learning, Introductory Data Mining and Data Science Project.\nWhen not in front of my computer I am reading, studying Japanese Kanji, or building mechanical keyboards. I also love rhythm games.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://juanpablos.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Master\u0026rsquo;s student at the University of Chile, interested in Software Engineering, Machine Learning and Data Science. My Master\u0026rsquo;s thesis is about the Logical Interpretability of Graph Neural Networks, figuring out what is happening inside the network in the domain of formal logic.\nMy experience consists mostly of research on uses of Machine Learning on source code and research of Graph Neural Networks, and implementing various prototypes for freelance and research projects.","tags":null,"title":"Juan-Pablo Silva","type":"authors"},{"authors":["Pablo Barceló","Egor V. Kostylev","Mikael Monet","Jorge Pérez","Juan Reutter","Juan-Pablo Silva"],"categories":null,"content":"","date":1576724400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576724400,"objectID":"54b89254a6482b42b17177c2091633ce","permalink":"https://juanpablos.github.io/publication/iclr20-logic/","publishdate":"2019-12-19T00:00:00-03:00","relpermalink":"/publication/iclr20-logic/","section":"publication","summary":"The ability of graph neural networks (GNNs) for distinguishing nodes in graphs has been recently characterized in terms of the Weisfeiler-Lehman (WL) test for checking graph isomorphism. This characterization, however, does not settle the issue of which Boolean node classifiers (i.e., functions classifying nodes in graphs as true or  false) can be expressed by GNNs.  We tackle this problem by focusing on Boolean classifiers expressible as formulas in the logic FOC2, a well-studied fragment of first order logic. FOC2 is tightly related to the WL test, and hence to GNNs. We start by studying a popular class of GNNs, which we call AC-GNNs, in which the features of each node in the graph are updated, in successive layers, only in terms of the features of its neighbors.  We show that this class of GNNs is too weak to capture all FOC2 classifiers, and provide a syntactic characterization of  the largest subclass of FOC2 classifiers that can be captured by AC-GNNs. This subclass coincides with a logic heavily used by the knowledge representation community. We then look at what needs to be added to AC-GNNs for capturing all FOC2 classifiers. We show that it suffices to add readout functions, which allow to update the features of a node not only in terms of its neighbors, but also in terms of a global attribute vector. We call GNNs of this kind ACR-GNNs. We experimentally validate our findings showing that, on synthetic data conforming to FOC2 formulas, AC-GNNs struggle to fit the training data while ACR-GNNs can generalize even to graphs of sizes not seen during training.","tags":[],"title":"The Logical Expressiveness of Graph Neural Networks","type":"publication"},{"authors":["Mikaël Monet","Jorge Pérez","Juan Reutter","Egor Kostylev","Pablo Barceló","Juan-Pablo Silva"],"categories":null,"content":"","date":1570935600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570935600,"objectID":"c2b9a518a23579047cdafb1ab8d0300a","permalink":"https://juanpablos.github.io/publication/glr19-logic/","publishdate":"2019-10-13T00:00:00-03:00","relpermalink":"/publication/glr19-logic/","section":"publication","summary":"The ability of graph neural networks (GNNs) for distinguishing nodes in graphs has been recently characterized in terms of the Weisfeiler-Lehman (WL) test for checking graph isomorphism. This characterization, however, does not settle the issue of which Boolean node classifiers (i.e., functions classifying nodes in graphs as true or  false) can be expressed by GNNs.  We tackle this problem by focusing on Boolean classifiers expressible as formulas in the logic FOC2, a well-studied fragment of first order logic. FOC2 is tightly related to the WL test, and hence to GNNs. We start by studying a popular class of GNNs, which we call AC-GNNs, in which the features of each node in the graph are updated, in successive layers, only in terms of the features of its neighbors.  We show that this class of GNNs is too weak to capture all FOC2 classifiers, and provide a syntactic characterization of  the largest subclass of FOC2 classifiers that can be captured by AC-GNNs. This subclass coincides with a logic heavily used by the knowledge representation community. We then look at what needs to be added to AC-GNNs for capturing all FOC2 classifiers. We show that it suffices to add readout functions, which allow to update the features of a node not only in terms of its neighbors, but also in terms of a global attribute vector. We call GNNs of this kind ACR-GNNs. We experimentally validate our findings showing that, on synthetic data conforming to FOC2 formulas, AC-GNNs struggle to fit the training data while ACR-GNNs can generalize even to graphs of sizes not seen during training.","tags":[],"title":"Logical Expressiveness of Graph Neural Networks","type":"publication"},{"authors":null,"categories":null,"content":"For the best experience, download the slides\u0026rsquo; code and open them with Jupyter Notebook so you can execute the code cells. You can also install RISE to see them as a slideshow.\n","date":1567453500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567453500,"objectID":"5d4b148a50cb2ddf75ce493d22b356cf","permalink":"https://juanpablos.github.io/talk/2019-09-02-python/","publishdate":"2019-09-02T16:45:00-03:00","relpermalink":"/talk/2019-09-02-python/","section":"talk","summary":"Aprende tópicos avanzados de Python como herencia multi-clase, super clases virtuales y metaclases.","tags":[],"title":"Advanced Python: the most pretentious title you will hear today (In Spanish)","type":"talk"}]